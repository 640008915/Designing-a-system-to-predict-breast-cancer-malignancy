{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import sort\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pandas import DataFrame\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from random import seed\n",
    "from random import randrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\Hannah\\Desktop\\Research Project\\data.csv\")\n",
    "data = data.drop([\"Unnamed: 32\", \"id\"],1)\n",
    "\n",
    "X = data.values[:,2:-1].astype('float64')\n",
    "X = (X - np.mean(X, axis =0)) /  np.std(X, axis = 0)\n",
    "X = np.hstack([np.ones((X.shape[0], 1)),X]) \n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "Y = data[\"diagnosis\"].map({'M':1,'B':0})\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_select = data[['texture_mean', 'smoothness_mean', 'compactness_mean',\n",
    "       'radius_se', 'symmetry_se', 'symmetry_mean', 'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
    "        'area_worst', 'smoothness_worst',\n",
    "       'compactness_worst', 'symmetry_worst']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 13)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_select, Y, test_size=0.25, random_state=0)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hypothesis(theta, x):   \n",
    "    return Sigmoid(x @ theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cost_Function(X,Y,theta,m):\n",
    "    hi = Hypothesis(theta, x)\n",
    "    _y = Y.reshape(-1, 1)\n",
    "    J = 1/float(m) * np.sum(-_y * np.log(hi) - (1-_y) * np.log(1-hi))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cost_Function_Derivative(X,Y,theta,m,alpha):\n",
    "    hi = Hypothesis(theta,X)\n",
    "    _y = Y.reshape(-1, 1)\n",
    "    J = alpha/float(m) * X.T @ (hi - _y)\n",
    "    return J\n",
    "\n",
    "def Gradient_Descent(X,Y,theta,m,alpha):\n",
    "    new_theta = theta - Cost_Function_Derivative(X,Y,theta,m,alpha)\n",
    "    return new_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hannah\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Accuracy with scikit-learn selected features:  84.6153846154 %\n",
      "Selected feature weights: [[ -1.40824150e+03]\n",
      " [ -1.64163044e+01]\n",
      " [  9.94034511e+00]\n",
      " [  9.42902171e+00]\n",
      " [ -3.71885853e+00]\n",
      " [ -2.89220341e+01]\n",
      " [ -3.20175737e-01]\n",
      " [ -2.09268486e+03]\n",
      " [ -7.36779661e+02]\n",
      " [  9.58164139e+01]\n",
      " [ -1.59250392e+01]\n",
      " [  6.27735636e+01]\n",
      " [ -2.81008644e+01]]\n"
     ]
    }
   ],
   "source": [
    "def Accuracy2(theta):\n",
    "    correct = 0\n",
    "    length = len(X_test)\n",
    "    prediction = (Hypothesis(theta, X_test) > 0.5) #replaced round to use predictions which are correct is ranging between the Y shape below\n",
    "    _y = Y_test.reshape(-1, 1)\n",
    "    correct = prediction == _y\n",
    "    my_accuracy = (np.sum(correct) / length)*100\n",
    "    print ('LR Accuracy with scikit-learn selected features: ', my_accuracy, \"%\")\n",
    "    \n",
    "def Logistic_Regression2(X,Y,alpha,theta,num_iters):\n",
    "    m = len(Y)\n",
    "    for x in range(num_iters):\n",
    "        new_theta = Gradient_Descent(X,Y,theta,m,alpha)\n",
    "        theta = new_theta\n",
    "        if x % 100 == 0:\n",
    "            print #('theta: ', theta)    \n",
    "            print #('cost: ', Cost_Function(X,Y,theta,m))\n",
    "    Accuracy2(theta)\n",
    "    print('Selected feature weights:', theta)\n",
    "    #x = np.linspace(-10, 30, 50)\n",
    "    #y = -(theta[0] + theta[1]*x)/theta[2]\n",
    "    #plt.plot(X_train, Y_train)\n",
    "    #plt.plot(theta)\n",
    "    #plt.show()\n",
    "    #idxs_selected = select.get_support(indices=True)\n",
    "    #print(idxs_selected)\n",
    "  \n",
    "ep = .012   #sets initial theta to random non zero numbers +/- .012 specifically is arbitrary\n",
    "\n",
    "initial_theta = np.random.rand(X_train.shape[1],1) * 2 * ep - ep\n",
    "alpha = 0.5\n",
    "iterations = 10000\n",
    "Logistic_Regression2(X_train,Y_train,alpha,initial_theta,iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hannah\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Accuracy with scikit-learn selected features:  83.2167832168 %\n",
      "Confusion Matrix: \n",
      " [[ 9 81]\n",
      " [15 38]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAD8CAYAAAA2Y2wxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEH1JREFUeJzt3XmwXGWZx/Hvkw1Qw5CQTAwJjiB7UCIwiLKoASQBNVFG\nBxRBjBOsKRZnxH0XFxxxEKcYpsKOYNgGBBmVEZCBKVlECREIyiJqCIYECGsScm8/80da6w65Sd9L\n+u3T9/D9pE7d26e73/Ok6tavnnrPe86JzESSVM6wqguQpLozaCWpMINWkgozaCWpMINWkgozaCWp\nMINWkgozaCWpMINWkgobUfwAoyZ56ZnWsmLxTVWXoC40ctzWsaFjrF724IAzpx3HGwg7WkkqrHhH\nK0kd1eituoK1GLSS6qW3p+oK1mLQSqqVzEbVJazFoJVULw2DVpLKsqOVpMI8GSZJhdnRSlJZ6aoD\nSSrMk2GSVJhTB5JUmCfDJKkwO1pJKsyTYZJUWJtOhkXE9sDFfXZtDXwB2Az4B2Bpc/9nMvNH6xvL\noJVUK5ntmaPNzN8AUwEiYjjwMHAFcBRwSmaePNCxDFpJ9VJmjnY/4IHM/H3E4O8V7o2/JdVLozHg\nLSLmRMTtfbY56xj1UGBen9fHRMSCiDg7Isa0Kikyyz5pxkfZqD8+ykb9acejZVb+8gcDzpyNd5vV\n8ngRMQpYDEzJzCURMQFYBiRwIjAxMz+0vjGcOpBUL72r2z3iDOBXmbkE4M8/ASLiDODqVgMYtJLq\npf2X4B5Gn2mDiJiYmY80X74LuKvVAAatpHpp48mwiHgZcABwdJ/d/xIRU1kzdfDQC97rl0ErqV7a\n2NFm5nPA5i/Y94HBjmPQSqoX794lSWVl+0+GbTCDVlK9eFMZSSrMqQNJKsyOVpIKs6OVpMLsaCWp\nsB5v/C1JZdnRSlJhztFKUmF2tJJUmB2tJBVmRytJhbnqQJIKK/x4rhfDoJVUL87RSlJhBq0kFebJ\nMEkqrLe36grWYtBKqhenDiSpMINWkgpzjlaSysqG62glqSynDiSpMFcdSFJhXdjRDqu6gJeKY4+Z\nzfw7ruPO+ddz3LEfrrocVeT8i65g5vuPZtbhH+HjXzyJVaue5/uXXcWM936InfeawRPLn6y6xKGv\n0Rj41iEGbQdMmbI9s2e/jze+6WB23e0ADj5of7bZZquqy1KHLVm6jAsvu5KLz/4uP7jgP2g0Gvz4\n2v/h9a/biTNP/QZbvPKvqy6xHjIHvnVIy6mDiNgBmAlMAhJYDFyVmQsL11YbO+ywLbfe+itWrFgJ\nwI033cKsmdM5+dunV1yZOq2nt5dVq55nxPARrFi5ivHjxrLjdttUXVa9DLWpg4j4JHAREMBtwC+a\nv8+LiE+VL68e7r77XvbZZ0/Gjh3DJptszIzp05g8eYuqy1KHTRg/jg8edgj7v/sI3jrzfYx++cvY\n6w27VV1W/TRy4FuHtOpoZwNTMnN1350R8a/A3cBJpQqrk3vvvZ9vfes0fvLjeTz7zLPcueAeenu6\n78yoynryqaf52U23cM2l5zB69Cv42Oe+zg+vuZ53HDit6tLqpQtXHbSao20A/bVeE5vv9Ssi5kTE\n7RFxe6Px7IbUVxvnnHsRe7xhOm/d7xCeeGI5993/u6pLUofdcvt8Jm0xgbFjNmPkiBHs9+Y3Mf/X\n91RdVu1kozHgrVNadbQfBa6LiPuAPzb3vQrYBjhmXV/KzLnAXIARoyZ132UaFRg/fnOWLn2MLbfc\nglmzZrD3Pu+suiR12MQJ41lw172sWLmSjTfaiFtvn8+UHbatuqz6GWpXhmXmTyJiO2AP1pwMC2AR\n8IvM7L7+vItdevEZjN18DKtX93DccZ9luct4XnJeN2UHDnjr3rz3qGMZPnw4O2z3Gt4zcwYXXHol\n51x4Kcsef4J3H/GP7PPGv+Urn/5o1eUOXV14r4PIwksc7GjVnxWLb6q6BHWhkeO2jg0d49mvvH/A\nmfPyL1y4wccbCK8Mk1QvXXii2QsWJNVLNga+tRARm0XEZRFxb0QsjIg3RsTYiPhpRNzX/Dmm1TgG\nraR6ae862lOBn2TmDsAuwELgU8B1mbktcF3z9XoZtJJqpV3LuyJiU2Bf4CyAzHw+M5ez5krZ85of\nOw+Y1aomg1ZSvbSvo90aWAqcExF3RMSZEfFyYEJmPgLQ/NnyJhUGraR6GUTQ9r24qrnN6TPSCGBX\n4PTMfD3wLAOYJuiPqw4k1csgLsHte3FVPxYBizLz1ubry1gTtEsiYmJmPhIRE4FHWx3HjlZSrWQj\nB7ytd5zMPwF/jIjtm7v2A+4BrgKObO47EriyVU12tJLqpb2X4B4LXBgRo4AHgaNY06BeEhGzgT8A\n72k1iEErqV7aeLOYzJwP7N7PW/sNZhyDVlK9DLWbykjSkGPQSlJZ2dt9d+8yaCXVix2tJJXVatlW\nFQxaSfVi0EpSYd03RWvQSqqX7Om+pDVoJdVL9+WsQSupXjwZJkml2dFKUll2tJJUmh2tJJWVPVVX\nsDaDVlKtDOAp4h1n0EqqF4NWksqyo5WkwgxaSSose6PqEtZi0EqqFTtaSSosG3a0klSUHa0kFZZp\nRytJRdnRSlJhDVcdSFJZngyTpMIMWkkqLLvvdrQGraR6saOVpMJc3iVJhfW66kCSyrKjlaTCnKOV\npMJcdSBJhdnRSlJhvY1hVZewlu6rSJI2QObAt4GIiOERcUdEXN18fW5E/C4i5je3qa3GsKOVVCuN\n9q86OB5YCGzaZ9/HM/OygQ5gRyupVjJjwFsrETEZOBg4c0NqMmgl1Uqbpw6+A3wCeOFdbr8WEQsi\n4pSI2KjVIMWnDrYcPa70ITQEXfy6L1RdgrrQ4Ysv2OAxBjN1EBFzgDl9ds3NzLnN994OPJqZv4yI\nt/T5zKeBPwGjgLnAJ4GvrO84ztFKqpXBrDpohurcdby9F/DOiDgI2BjYNCIuyMzDm++viohzgBNa\nHcepA0m1koPY1jtO5qczc3Jmvho4FLg+Mw+PiIkAERHALOCuVjXZ0UqqlQKrDl7owogYDwQwH/hI\nqy8YtJJqpcRNZTLzBuCG5u/TBvt9g1ZSrXThQ3ANWkn1knivA0kqqsf70UpSWXa0klSYc7SSVJgd\nrSQVZkcrSYX12tFKUlld+CQbg1ZSvTTsaCWprC58CK5BK6lePBkmSYU1wqkDSSqqt+oC+mHQSqoV\nVx1IUmGuOpCkwlx1IEmFOXUgSYW5vEuSCuu1o5WksuxoJakwg1aSCuvCR4YZtJLqxY5WkgrzElxJ\nKsx1tJJUmFMHklSYQStJhXmvA0kqzDlaSSrMVQeSVFijCycPDFpJteLJMEkqrPv6WYNWUs3Y0UpS\nYT3RfT2tQSupVrovZmFY1QVIUjs1BrGtT0RsHBG3RcSdEXF3RHy5uX+riLg1Iu6LiIsjYlSrmgxa\nSbXSIAe8tbAKmJaZuwBTgekRsSfwTeCUzNwWeAKY3Wogg1ZSreQgtvWOs8YzzZcjm1sC04DLmvvP\nA2a1qsmglVQrg5k6iIg5EXF7n21O37EiYnhEzAceBX4KPAAsz8ye5kcWAZNa1eTJMEm10juI02GZ\nOReYu573e4GpEbEZcAWwY38fa3Ucg1ZSrZRYR5uZyyPiBmBPYLOIGNHsaicDi1t936kDSbWSg/i3\nPhExvtnJEhGbAPsDC4GfAX/X/NiRwJWtarKjlVQrbexoJwLnRcRw1jSll2Tm1RFxD3BRRHwVuAM4\nq9VABm0h3/zul5n2tn15bNnjTN/7EACO/8RHOPSIQ3h82eMAfOur/8YN1/5vlWWqw4ZtNJK3Xf45\nho8aQYwYzh/+6zYWnHw5r9x7Crt+/jAYFvQ8u5Kff3Quzzy0pOpyh6R23b0rMxcAr+9n/4PAHoMZ\ny6At5D/nXcn5Z87j2//+tf+3/+zTv8cZp51fUVWqWmPVaq59z9fpeW4VMWI4B/7g8yy+/k72+MYH\nueGoU3jq/sVsd+T+vPb4mdz8T+s8R6P16MYrwwzaQm67+VdM2nKLqstQF+p5bhUAw0YOZ9jIEWQz\nGUaO3uQvP1csWV5VeUNeTxdG7YsO2og4KjPPaWcxLwVHfPhQ3v3372DB/Hv42udP5qknn666JHVY\nDAtmXPNVRr96Ar8996c8dscD3PyxM5n2vRPoWbma1c+s4Jq3f6nqMoesVie5qrAhqw6+vK43+i4C\nfnrlYxtwiHq58JxLePNub+egN7+XpUuW8tkTT6i6JFUgG8mPDvgsl+92HJtPfQ1/tf1kdpwznes/\ncDJX7H4cD158I7t96f1VlzlkteteB+203qCNiAXr2H4NTFjX9zJzbmbunpm7j95487YXPVQtW/o4\njUaDzGTe+Zezy647V12SKrT6qedYcvNCJk3bhTE7vYrH7ngAgIeuuoVxu29bcXVDV7uWd7VTq6mD\nCcCBrLlxQl8B/LxIRTU2fsI4li5ZBsCBB0/jtwvvr7giddpGY0fT6Oll9VPPMXzjkUzcZ2fuPu2H\njNz0ZYze+pU8/eCfmLjvzjx138NVlzpkDcUbf18NvCIz57/wjeZVElqHU+eexJ577c6YzTfj57/+\nb75z0unsuffu7Ljz9pDJoj8s5jMfO7HqMtVhm0zYjDedejQxbBgxLPj9D2/l4Wvnc+sJZ7HvGcdD\no8HzTz7Hzf/sioMXqze7b442snBRW22+S/f9r1W5Ezd6bdUlqAsdvviC2NAx3vc37xpw5nz/91ds\n8PEGwuVdkmqlG1cdGLSSamUoztFK0pDSrktw28mglVQrTh1IUmHduOrAoJVUK04dSFJhngyTpMKc\no5Wkwpw6kKTCSl/t+mIYtJJqZTCPG+8Ug1ZSrTh1IEmFOXUgSYXZ0UpSYS7vkqTCvARXkgpz6kCS\nCjNoJakwVx1IUmF2tJJUmKsOJKmw3uy+GyUatJJqxTlaSSrMOVpJKsw5WkkqrOHUgSSVZUcrSYV1\n46qDYVUXIEnt1Mgc8NZKRJwdEY9GxF199n0pIh6OiPnN7aBW4xi0kmolB/FvAM4Fpvez/5TMnNrc\nftRqEKcOJNVKO0+GZeaNEfHqDR3HjlZSrbS5o12XYyJiQXNqYUyrDxu0kmqlN3sHvEXEnIi4vc82\nZwCHOB14DTAVeAT4dqsvOHUgqVYGcwluZs4F5g5y/CV//j0izgCubvUdg1ZSrZS+BDciJmbmI82X\n7wLuWt/nwaCVVDPtvKlMRMwD3gKMi4hFwBeBt0TEVCCBh4CjW41j0EqqlTavOjisn91nDXYcg1ZS\nrXgJriQV1o2X4Bq0kmrFG39LUmHeJlGSCrOjlaTCfJSNJBVmRytJhbnqQJIK82SYJBXm1IEkFeaV\nYZJUmB2tJBXWjXO00Y3pX1cRMad5o2HpL/y7qD8fZdNZA3lMhl56/LuoOYNWkgozaCWpMIO2s5yH\nU3/8u6g5T4ZJUmF2tJJUmEHbIRExPSJ+ExH3R8Snqq5H1YuIsyPi0Yho+bhqDW0GbQdExHDgNGAG\nsBNwWETsVG1V6gLnAtOrLkLlGbSdsQdwf2Y+mJnPAxcBMyuuSRXLzBuBx6uuQ+UZtJ0xCfhjn9eL\nmvskvQQYtJ0R/exzuYf0EmHQdsYiYMs+rycDiyuqRVKHGbSd8Qtg24jYKiJGAYcCV1Vck6QOMWg7\nIDN7gGOAa4CFwCWZeXe1ValqETEPuBnYPiIWRcTsqmtSGV4ZJkmF2dFKUmEGrSQVZtBKUmEGrSQV\nZtBKUmEGrSQVZtBKUmEGrSQV9n+uNAv4aBe/6QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29e04c6dcf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def Accuracy2(theta):\n",
    "    correct = 0\n",
    "    length = len(X_test)\n",
    "    prediction = (Hypothesis(theta, X_test) > 0.5) #replaced round to use predictions which are correct is ranging between the Y shape below\n",
    "    _y = Y_test.reshape(-1, 1)\n",
    "    correct = prediction == _y\n",
    "    my_accuracy = (np.sum(correct) / length)*100\n",
    "    print ('LR Accuracy with scikit-learn selected features: ', my_accuracy, \"%\")\n",
    "\n",
    "def cf(theta):\n",
    "    correct = 0\n",
    "    length = len(X_test)\n",
    "    prediction = (Hypothesis(theta, X_test) > 0.5) #replaced round to use predictions which are correct is ranging between the Y shape below\n",
    "    _y = Y_test.reshape(-1, 1)\n",
    "    correct = prediction == _y\n",
    "    cm=confusion_matrix(Y_test, correct)\n",
    "    print(\"Confusion Matrix:\", \"\\n\", confusion_matrix(Y_test, correct))\n",
    "    sns.heatmap(cm,annot=True,fmt=\"d\")\n",
    "\n",
    "\n",
    "    \n",
    "def Logistic_Regression2(X,Y,alpha,theta,num_iters):\n",
    "    m = len(Y)\n",
    "    for x in range(num_iters):\n",
    "        new_theta = Gradient_Descent(X,Y,theta,m,alpha)\n",
    "        theta = new_theta\n",
    "        if x % 100 == 0:\n",
    "            print #('theta: ', theta)    \n",
    "            print #('cost: ', Cost_Function(X,Y,theta,m))\n",
    "    Accuracy2(theta)\n",
    "    cf(theta)\n",
    "    \n",
    "  \n",
    "ep = .012   #sets initial theta to random non zero numbers +/- .012 specifically is arbitrary\n",
    "\n",
    "initial_theta = np.random.rand(X_train.shape[1],1) * 2 * ep - ep\n",
    "alpha = 0.5\n",
    "iterations = 10000\n",
    "Logistic_Regression2(X_train,Y_train,alpha,initial_theta,iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'nsplits'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-a9b946b8b2c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStratifiedShuffleSplit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnsplits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'nsplits'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "KFold(nsplits=4, shuffle=True).splits(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_split(dataset, folds=10):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / folds)\n",
    "    for i in range(folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "seed(1)\n",
    "dataset = X_train\n",
    "folds = cross_validation_split(dataset, 4)\n",
    "print(folds) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
